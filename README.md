# MathSolverAgent

Agente para resolu√ß√£o de problemas de matem√°tica avan√ßada com pipeline **inteiramente generativo**.
A LLM analisa o problema, converte para chamadas de ferramentas e explica a solu√ß√£o; as bibliotecas matem√°ticas executam somente os c√°lculos.

## Vis√£o geral

- Pipeline: `analysis -> converter -> solving -> verification`.
- Modo estrito de IA: sem LLM dispon√≠vel, o agente falha em `failed_precondition`.
- Suporte multimodal: texto, imagem, ou texto + imagem.
- API FastAPI + WebSocket + UI Streamlit.
- Checkpointing de sess√£o para retomada.

## Arquitetura

```text
math_solver_agent/
‚îú‚îÄ‚îÄ configs/
‚îÇ   ‚îú‚îÄ‚îÄ graph_config.yml
‚îÇ   ‚îî‚îÄ‚îÄ prompts.yml
‚îú‚îÄ‚îÄ src/
‚îÇ   ‚îú‚îÄ‚îÄ agents/
‚îÇ   ‚îÇ   ‚îú‚îÄ‚îÄ graph.py
‚îÇ   ‚îÇ   ‚îî‚îÄ‚îÄ state.py
‚îÇ   ‚îú‚îÄ‚îÄ api/
‚îÇ   ‚îÇ   ‚îî‚îÄ‚îÄ server.py
‚îÇ   ‚îú‚îÄ‚îÄ llm/
‚îÇ   ‚îÇ   ‚îî‚îÄ‚îÄ client.py
‚îÇ   ‚îú‚îÄ‚îÄ nodes/
‚îÇ   ‚îÇ   ‚îú‚îÄ‚îÄ analyzer.py
‚îÇ   ‚îÇ   ‚îú‚îÄ‚îÄ converter.py
‚îÇ   ‚îÇ   ‚îú‚îÄ‚îÄ solver.py
‚îÇ   ‚îÇ   ‚îî‚îÄ‚îÄ verifier.py
‚îÇ   ‚îú‚îÄ‚îÄ tools/
‚îÇ   ‚îÇ   ‚îú‚îÄ‚îÄ calculator.py
‚îÇ   ‚îÇ   ‚îú‚îÄ‚îÄ plotter.py
‚îÇ   ‚îÇ   ‚îî‚îÄ‚îÄ utils.py
‚îÇ   ‚îú‚îÄ‚îÄ ui/
‚îÇ   ‚îÇ   ‚îú‚îÄ‚îÄ api_client.py
‚îÇ   ‚îÇ   ‚îú‚îÄ‚îÄ chainlit_app.py
‚îÇ   ‚îÇ   ‚îî‚îÄ‚îÄ streamlit_app.py
‚îÇ   ‚îî‚îÄ‚îÄ main.py
‚îú‚îÄ‚îÄ tests/
‚îú‚îÄ‚îÄ docs/
‚îú‚îÄ‚îÄ .env.example
‚îî‚îÄ‚îÄ pyproject.toml
```

## Requisitos

- Python `>=3.13`
- Chave NVIDIA (`NVIDIA_API_KEY`) ou MARITACA (`MARITACA_API_KEY`)
- Depend√™ncias do projeto

Instala√ß√£o:

```bash
python3 -m venv .venv
source .venv/bin/activate
pip install -e .[dev]
```

Configura√ß√£o de ambiente:

```bash
cp .env.example .env
# editar .env e definir NVIDIA_API_KEY=...
```

## Sele√ß√£o de modelo (flex√≠vel)

A sele√ß√£o do modelo fica na se√ß√£o `llm` do `configs/graph_config.yml`.

Campos principais:

- `model_profile`: alias do perfil (recomendado)
- `model`: id expl√≠cito do modelo (opcional; sobrescreve o id do perfil)
- `temperature`, `top_p`, `max_tokens`: overrides de amostragem
- `chat_template_kwargs`: kwargs espec√≠ficos por modelo
- `multimodal_enabled`: habilita imagem quando o modelo suporta

Perfis prontos inclu√≠dos:

- `kimi_k2_5` -> `moonshotai/kimi-k2.5` (multimodal)
- `deepseek_v3_2` -> `deepseek-ai/deepseek-v3.2` (n√£o multimodal)
- `glm4_7` -> `z-ai/glm4.7` (multimodal)
- `glm5` -> `z-ai/glm5` (multimodal)
- `minimax_m2_1` -> `minimaxai/minimax-m2.1` (multimodal)
- `sabia_4` -> `sabia-4` (Maritaca, n√£o multimodal)
- `sabiazinho_4` -> `sabiazinho-4` (Maritaca, n√£o multimodal)

Exemplo r√°pido (DeepSeek):

```yaml
llm:
  model_profile: deepseek_v3_2
  model: deepseek-ai/deepseek-v3.2
  temperature: 1.0
  top_p: 0.95
  max_tokens: 8192
  multimodal_enabled: true # ser√° automaticamente efetivo como false para esse modelo
```

Exemplo r√°pido (Maritaca):

```yaml
llm:
  provider: maritaca
  model_profile: sabiazinho_4
  api_key_env: MARITACA_API_KEY
  temperature: 0.7
  max_tokens: 8192
  multimodal_enabled: true # ser√° automaticamente efetivo como false (modelo text-to-text)
```

## Execu√ß√£o

### CLI

Somente texto:

```bash
python3 -m src.main --mode cli --problem "Calcule a derivada de x^3"
```

Somente imagem:

```bash
python3 -m src.main --mode cli --image-path ./problema.png
```

Texto + imagem:

```bash
python3 -m src.main --mode cli --problem "Resolva" --image-path ./problema.png
```

Retomada de sess√£o:

```bash
python3 -m src.main --mode cli --problem "2+2" --session-id demo
python3 -m src.main --mode cli --resume --session-id demo
```

### API FastAPI

```bash
python3 -m src.main --mode api --host 0.0.0.0 --port 8000
```

Endpoints:

- `GET /health`
- `POST /v1/solve`
- `WS /v1/solve/stream`
- `GET /v1/runtime/status`
- `POST /v1/jobs/solve`
- `GET /v1/jobs/{job_id}`
- `DELETE /v1/jobs/{job_id}`
- `POST /v1/export`

Exemplo `POST /v1/solve`:

```bash
curl -X POST "http://localhost:8000/v1/solve" \
  -H "Content-Type: application/json" \
  -d '{
    "problem": "Calcule a derivada de x^3",
    "session_id": "sessao-001",
    "resume": false
  }'
```

Exemplo com override por requisi√ß√£o (provider/model):

```bash
curl -X POST "http://localhost:8000/v1/solve" \
  -H "Content-Type: application/json" \
  -d '{
    "problem": "Calcule a derivada de x^3",
    "provider": "maritaca",
    "model_profile": "sabiazinho_4",
    "temperature": 0.7,
    "max_tokens": 8192
  }'
```

Observa√ß√£o: a API resolve automaticamente `api_key_env` com base no `provider` quando esse campo n√£o √© enviado.

Exemplo async (jobs):

```bash
curl -X POST "http://localhost:8000/v1/jobs/solve" \
  -H "Content-Type: application/json" \
  -d '{"problem":"Calcule a derivada de x^3"}'
```

```bash
curl "http://localhost:8000/v1/jobs/<job_id>"
```

Exemplo multimodal (`image_url`):

```bash
curl -X POST "http://localhost:8000/v1/solve" \
  -H "Content-Type: application/json" \
  -d '{
    "problem": "",
    "image_url": "https://exemplo.com/problema.png",
    "image_media_type": "image/png"
  }'
```

### UI Streamlit

Terminal 1 (API):

```bash
python3 -m src.main --mode api --host 0.0.0.0 --port 8000
```

Terminal 2 (UI):

```bash
streamlit run src/ui/streamlit_app.py
```

A UI permite:

- enviar somente texto;
- enviar somente imagem;
- enviar texto + imagem;
- escrever o enunciado em Markdown/LaTeX.

### UI Chainlit (chat de agentes)

Terminal 1 (API):

```bash
python3 -m src.main --mode api --host 0.0.0.0 --port 8001
```

Terminal 2 (UI):

```bash
chainlit run src/ui/chainlit_app.py -w
```

No Chainlit, mantenha `API Base URL` apontando para a API FastAPI (padrao: `http://localhost:8001`).

`timeout_seconds` na UI Chainlit vai de `60` a `1000` segundos (padrao `600`).
A lista de `model_profile` exibe todos os perfis do `graph_config.yml`; a combinacao efetiva `provider/profile` e validada no backend.

Para debug do frontend Chainlit, ajuste o nivel de log:

```bash
MATH_SOLVER_UI_LOG_LEVEL=INFO chainlit run src/ui/chainlit_app.py -w
```

Os logs exibem `request_id`, URL da API, tempo de chamada e status, ajudando a identificar onde ocorre timeout.

Se o Chainlit reclamar que `.chainlit/config.toml` esta desatualizado, remova a pasta local e rode novamente:

```bash
rm -rf .chainlit
chainlit run src/ui/chainlit_app.py -w
```

A UI Chainlit permite:

- chat estilo assistente com render de Markdown/LaTeX;
- upload de imagem (PNG/JPG) pelo clipe na caixa de mensagem;
- sele√ß√£o de `provider`, `model_profile`, `temperature`, `max_tokens` e `session_id`;
- exibi√ß√£o de `decision_trace` antes da resposta final;
- consulta de status operacional (`runtime status`) e feedback de fila/ocupa√ß√£o;
- execu√ß√£o padr√£o via jobs ass√≠ncronos com polling de progresso (`queued/running/succeeded`);
- comando `/resume` para retomar checkpoint da sess√£o ativa.
- envio de overrides para `/v1/solve` sem precisar informar `api_key_env` (resolvido pelo backend).

## Modo estrito generativo

No `configs/graph_config.yml`, se√ß√£o `llm`:

- `enabled: true`
- `require_available: true`

Com isso:

- sem LLM dispon√≠vel, o agente retorna `failed_precondition`;
- nos endpoints REST (`/v1/solve` e `/v1/export`), a API retorna HTTP `503`.

Resili√™ncia operacional adicional:

- `429` quando o runtime est√° ocupado (limites de concorr√™ncia/fila);
- `504` quando o request excede `solve_hard_timeout_seconds`;
- monitoramento em `GET /v1/runtime/status`.

## Testes

Principal (unittest):

```bash
python3 -m unittest discover -s tests -v
```

Opcional (pytest):

```bash
pytest
```

## Documenta√ß√£o complementar

- API: `docs/API.md`
- Opera√ß√£o e troubleshooting: `docs/OPERATIONS.md`

## üì¨ Contato

Projeto mantido por Thiago Bibiano. Para d√∫vidas, sugest√µes ou colabora√ß√£o, entre em contato:

üîó LinkedIn: https://www.linkedin.com/in/thiago-bibiano-da-silva-510b3b15b/
